# LMDFormer

We are organizing the code and will upload it soon

Abstract: Vision Transformer (ViT) models are notably powerful yet demand substantial computational resources, hindering its deployment on mobile devices with constrained computational capabilities. Existing lightweight models achieve parameter reduction through the implementation of a CNN-Transformer hybrid architecture or modifications to Transformer strategies. In this work, we introduce LMDFormer, an innovative lightweight hybrid CNN-ViT model specifically designed for object detection. In LMDFormer, we propose a novel multi-dimensional self-attention encoderâ€”the lightweight multi-dimensional attention (LMDA) encoder, designed to capture full-dimensional features. It achieves this through a parallel attention architecture operating both space-wise and channel-wise on sparse feature maps. This approach ensures comprehensive utilization of the information across each dimension of the feature map, thereby implicitly expanding the receptive field to encompass full dimensions under restricted computational resources. The hybrid model LMDFormer is composed of a specially designed CNN-based lightweight convolution module (LCM) and LMDA encoder, which effectively establish connections between local and global features. In object detection task, LMDFormer exhibits superior performance, achieving a 28.9 mAP with 2.1G MAdds on the COCO dataset, thereby enhancing accuracy by 4.3% relative to MobileViT while consuming less computational power.
